# 🚀 Large Language Models – From Scratch (IITM Course Assignments)

Welcome! This repository showcases my end-to-end implementations of transformer-based architectures completed as part of the **"Large Language Models" course by Prof. Mitesh Khapra (IIT Madras)**.

Every assignment is implemented **from scratch using PyTorch**, without relying on built-in transformer modules — demonstrating my deep understanding of attention mechanisms, model architecture, and modern language modeling techniques.

---

## 📘 Assignment Highlights

### 1. 🔧 Transformer Encoder
- Implemented multi-head self-attention (MHA)
- Added positional encoding and FFN
- Predicts token IDs from encoder output

### 2. 🔄 Transformer Decoder
- Built masked self-attention and encoder-decoder cross attention
- Implemented causal language modeling (CLM) objective

### 3. 🌐 Full Transformer – Machine Translation
- Combined encoder and decoder to translate simple English sentences to Tamil
- Demonstrated full sequence-to-sequence modeling

### 4. 🧠 GPT-style Language Model
- Decoder-only transformer trained with CLM
- Used ReLU activations as specified (not GELU)

### 5. 🧬 BERT-style Language Model
- Encoder-only transformer trained using masked language modeling (MLM)
- Pretrained architecture for bidirectional understanding

---

## 🛠 Technical Skills Demonstrated

- ✅ Attention mechanisms (MHA, masking, cross-attention)
- ✅ Language modeling (CLM, MLM)
- ✅ Custom model building with PyTorch
- ✅ Data loading, batching, and optimization
- ✅ End-to-end training workflows

---

## 💼 Why This Matters

This project reflects my:
- 🔍 Deep understanding of Transformer internals  
- 🧠 Strong implementation skills in PyTorch  
- 🧪 Ability to build complex models without relying on high-level libraries  
- 📊 Readiness to contribute to real-world LLM systems, research, and production ML

---

├── assignment_1_transformer_encoder.ipynb
├── assignment_2_transformer_decoder.ipynb
├── assignment_3_machine_translation.ipynb
├── assignment_4_gpt_causal_lm.ipynb
└── assignment_5_bert_mlm.ipynb

