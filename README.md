# ğŸš€ Large Language Models â€“ From Scratch (IITM Course Assignments)

Welcome! This repository showcases my end-to-end implementations of transformer-based architectures completed as part of the **"Large Language Models" course by Prof. Mitesh Khapra (IIT Madras)**.

Every assignment is implemented **from scratch using PyTorch**, without relying on built-in transformer modules â€” demonstrating my deep understanding of attention mechanisms, model architecture, and modern language modeling techniques.

---

## ğŸ“˜ Assignment Highlights

### 1. ğŸ”§ Transformer Encoder
- Implemented multi-head self-attention (MHA)
- Added positional encoding and FFN
- Predicts token IDs from encoder output

### 2. ğŸ”„ Transformer Decoder
- Built masked self-attention and encoder-decoder cross attention
- Implemented causal language modeling (CLM) objective

### 3. ğŸŒ Full Transformer â€“ Machine Translation
- Combined encoder and decoder to translate simple English sentences to Tamil
- Demonstrated full sequence-to-sequence modeling

### 4. ğŸ§  GPT-style Language Model
- Decoder-only transformer trained with CLM
- Used ReLU activations as specified (not GELU)

### 5. ğŸ§¬ BERT-style Language Model
- Encoder-only transformer trained using masked language modeling (MLM)
- Pretrained architecture for bidirectional understanding

---

## ğŸ›  Technical Skills Demonstrated

- âœ… Attention mechanisms (MHA, masking, cross-attention)
- âœ… Language modeling (CLM, MLM)
- âœ… Custom model building with PyTorch
- âœ… Data loading, batching, and optimization
- âœ… End-to-end training workflows

---

## ğŸ’¼ Why This Matters

This project reflects my:
- ğŸ” Deep understanding of Transformer internals  
- ğŸ§  Strong implementation skills in PyTorch  
- ğŸ§ª Ability to build complex models without relying on high-level libraries  
- ğŸ“Š Readiness to contribute to real-world LLM systems, research, and production ML

---

â”œâ”€â”€ assignment_1_transformer_encoder.ipynb
â”œâ”€â”€ assignment_2_transformer_decoder.ipynb
â”œâ”€â”€ assignment_3_machine_translation.ipynb
â”œâ”€â”€ assignment_4_gpt_causal_lm.ipynb
â””â”€â”€ assignment_5_bert_mlm.ipynb

