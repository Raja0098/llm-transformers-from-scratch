{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* The objective of this assignments is to build the **Decoder** part of the Transformer architecture.\n",
        "* We will be using the **PyTorch** framework to implement the following components\n",
        "  * Decoder Layer that contains\n",
        "    * Multi-Head Masked Attention (MHMA) Module\n",
        "    * Multi-Head Cross Attention (MHMA) Module\n",
        "    * Position-wise Feed Forward Neural Network\n",
        "\n",
        "  * Implement CLM\n",
        "\n",
        "* **DO NOT** USE Built-in **TRANSFORMER LAYERS** as it affects the reproducibility.\n",
        "\n",
        "* You will be given with a configuration file that contains information on various hyperparameters such as embedding dimension, vocabulary size,number heads and so on\n",
        "\n",
        "* Use ReLU activation function and Stochastic Gradient Descent optimizer\n",
        "* Here are a list of helpful Pytorch functions (does not mean you have to use all of them) for this subsequent assignments\n",
        "  * [torch.matmul](https://pytorch.org/docs/stable/generated/torch.matmul.html#torch-matmul)\n",
        "  * [torch.bmm](https://pytorch.org/docs/stable/generated/torch.bmm.html)\n",
        "  * torch.swapdims\n",
        "  * torch.unsqueeze\n",
        "  * torch.squeeze\n",
        "  * torch.argmax\n",
        "  * [torch.Tensor.view](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html)\n",
        "  * [torch.nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
        "  * [torch.nn.Parameter](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html)\n",
        "  * torch.nn.Linear\n",
        "  * torch.nn.LayerNorm\n",
        "  * torch.nn.ModuleList\n",
        "  * torch.nn.Sequential\n",
        "  * [torch.nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
        "  \n",
        "* Important: Do not set any global seeds.\n",
        "\n",
        "* Helpful resources to get started with\n",
        "\n",
        " * [Andrej Karpathys Nano GPT](https://github.com/karpathy/nanoGPT)\n",
        " * [PyTorch Source code of Transformer Layer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)\n",
        "\n"
      ],
      "metadata": {
        "id": "SLKgMhMp8wvg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "es7WIeXF3nrV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.functional import one_hot\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "from  pprint import pprint\n",
        "from yaml import safe_load\n",
        "import copy\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#do not edit this cell\n",
        "config_url = \"https://raw.githubusercontent.com/Arunprakash-A/LLM-from-scratch-PyTorch/main/config_files/dec_config.yml\"\n",
        "response = requests.get(config_url)\n",
        "config = response.content.decode(\"utf-8\")\n",
        "config = safe_load(config)\n",
        "pprint(config)"
      ],
      "metadata": {
        "id": "de2_k13-3_gS",
        "outputId": "1ff8925c-770f-4d66-d48a-8db4163f8425",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input': {'batch_size': 10, 'embed_dim': 32, 'seq_len': 8, 'vocab_size': 12},\n",
            " 'model': {'d_ff': 128,\n",
            "           'd_model': 32,\n",
            "           'dk': 4,\n",
            "           'dq': 4,\n",
            "           'dv': 4,\n",
            "           'n_heads': 8,\n",
            "           'n_layers': 6}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = config['input']['vocab_size']\n",
        "batch_size = config['input']['batch_size']\n",
        "seq_len = config['input']['seq_len']\n",
        "embed_dim = config['input']['embed_dim']\n",
        "dmodel = embed_dim\n",
        "dq = torch.tensor(config['model']['dq'])\n",
        "dk = torch.tensor(config['model']['dk'])\n",
        "dv = torch.tensor(config['model']['dv'])\n",
        "heads = torch.tensor(config['model']['n_heads'])\n",
        "d_ff = config['model']['d_ff']"
      ],
      "metadata": {
        "id": "kS7dciwb53M1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Input tokens"
      ],
      "metadata": {
        "id": "6uLkQpFA5-Gs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Generate a raw_input ids (without any special tokens appended to it)\n",
        "\n",
        "* Since we will be using this as label after adding the special  \\<start\\> token, we use the variable name \"label_ids\"\n",
        "\n",
        "* Keep the size of the `label_ids=(bs,seq_len-1)` as we insert a special token ids in the next step"
      ],
      "metadata": {
        "id": "PfUUsznGp9Ll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# do not edit this cell\n",
        "data_url = 'https://github.com/Arunprakash-A/LLM-from-scratch-PyTorch/raw/main/config_files/w2_input_tokens'\n",
        "r = requests.get(data_url)\n",
        "label_ids = torch.load(BytesIO(r.content),weights_only=True)\n",
        "print(label_ids, label_ids.size())"
      ],
      "metadata": {
        "id": "I_D8hvdH5_1E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6067676-d58f-42f8-f1d4-38148ca473f3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 7,  8,  7,  7,  9,  2,  6],\n",
            "        [10,  1, 10,  5,  3,  6,  8],\n",
            "        [ 3,  4,  8,  2, 10, 10, 10],\n",
            "        [ 4, 10,  1,  3,  4,  9,  7],\n",
            "        [ 8,  4,  7,  3,  8, 10,  5],\n",
            "        [ 9,  1,  8,  5,  9,  9, 10],\n",
            "        [ 7,  3,  8,  2,  5,  1,  5],\n",
            "        [ 3,  3,  2,  1,  4,  1,  1],\n",
            "        [10,  9,  9,  9,  6,  9,  2],\n",
            "        [ 3,  6,  6,  3,  5,  4,  5]]) torch.Size([10, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let the first token_id be be a special `[start]` token (mapped to integer 0)\n",
        "* If label_ids=$\\begin{bmatrix}1&2\\\\3&4 \\end{bmatrix}$, then we modify it as $\\begin{bmatrix}0&1&2\\\\0&3&4 \\end{bmatrix}$"
      ],
      "metadata": {
        "id": "nxmtUf5j6JJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#token_ids = None # the first column of token_ids should be zeros and the rest of the columns come from label_ids\n",
        "\n",
        "# Insert a special [start] token (ID = 0) at the beginning of label_ids\n",
        "start_token = torch.zeros(label_ids.shape[0], 1, dtype=torch.long)\n",
        "token_ids = torch.cat((start_token, label_ids), dim=1)\n",
        "\n",
        "print(token_ids, token_ids.size())"
      ],
      "metadata": {
        "id": "ADbTWVVC6fu-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0320e20a-3133-4269-b89a-17db1184c5b6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0,  7,  8,  7,  7,  9,  2,  6],\n",
            "        [ 0, 10,  1, 10,  5,  3,  6,  8],\n",
            "        [ 0,  3,  4,  8,  2, 10, 10, 10],\n",
            "        [ 0,  4, 10,  1,  3,  4,  9,  7],\n",
            "        [ 0,  8,  4,  7,  3,  8, 10,  5],\n",
            "        [ 0,  9,  1,  8,  5,  9,  9, 10],\n",
            "        [ 0,  7,  3,  8,  2,  5,  1,  5],\n",
            "        [ 0,  3,  3,  2,  1,  4,  1,  1],\n",
            "        [ 0, 10,  9,  9,  9,  6,  9,  2],\n",
            "        [ 0,  3,  6,  6,  3,  5,  4,  5]]) torch.Size([10, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement the following components of a decoder layer\n",
        "\n",
        " * Multi-head Masked Attention (MHMA)\n",
        " * Multi-head Cross Attention (MHCA)\n",
        " * Postion-wise FFN\n",
        "\n",
        "* Randomly initialize the parameters using normal distribution with the following seed values\n",
        "  * $W_Q:$(seed=43)\n",
        "  * $W_K:$(seed=44)\n",
        "  * $W_V:$(seed=45)\n",
        "  * $W_O:$(seed=46)\n",
        "\n",
        "* Remember that, Multi-head cross atention takes two represnetation. One is the encoder output and the other one is the output from masked attetnion sub-layer.\n",
        "\n",
        "* However, in this assignment, we will fix it to a random matrix."
      ],
      "metadata": {
        "id": "k8dvK-TdNNlH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MHMA\n",
        "\n",
        "* By default, `mask=None`. Therefore, create and apply the mask while computing the attention scores\n"
      ],
      "metadata": {
        "id": "87ZSIzbANWPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MHMA(nn.Module):\n",
        "    def __init__(self, dmodel, dq, dk, dv, heads, mask=None):\n",
        "        super(MHMA, self).__init__()\n",
        "\n",
        "        self.dmodel = dmodel\n",
        "        self.dq = dq\n",
        "        self.dk = dk\n",
        "        self.dv = dv\n",
        "        self.heads = heads\n",
        "\n",
        "        # Initialize weights for Q, K, V, and output using random seeds\n",
        "        torch.manual_seed(43)\n",
        "        self.WQ = nn.Parameter(torch.randn(dq * heads, dmodel))\n",
        "\n",
        "        torch.manual_seed(44)\n",
        "        self.WK = nn.Parameter(torch.randn(dk * heads, dmodel))\n",
        "\n",
        "        torch.manual_seed(45)\n",
        "        self.WV = nn.Parameter(torch.randn(dv * heads, dmodel))\n",
        "\n",
        "        torch.manual_seed(46)\n",
        "        self.WO = nn.Parameter(torch.randn(dmodel, dv * heads))\n",
        "\n",
        "        self.mask = mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Linear transformations for Q, K, V\n",
        "        Q = torch.matmul(x, self.WQ.T)  # (batch_size, seq_len, dq * heads)\n",
        "        K = torch.matmul(x, self.WK.T)  # (batch_size, seq_len, dk * heads)\n",
        "        V = torch.matmul(x, self.WV.T)  # (batch_size, seq_len, dv * heads)\n",
        "\n",
        "        # Reshape Q, K, V for multi-head computation\n",
        "        batch_size = Q.shape[0]\n",
        "        seq_len = Q.shape[1]\n",
        "\n",
        "        Q = Q.view(batch_size, seq_len, self.heads, self.dq).transpose(1, 2)  # (batch_size, heads, seq_len, dq)\n",
        "        K = K.view(batch_size, seq_len, self.heads, self.dk).transpose(1, 2)  # (batch_size, heads, seq_len, dk)\n",
        "        V = V.view(batch_size, seq_len, self.heads, self.dv).transpose(1, 2)  # (batch_size, heads, seq_len, dv)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        dk = torch.tensor(self.dk, dtype=torch.float32)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(dk)  # (batch_size, heads, seq_len, seq_len)\n",
        "\n",
        "        # Apply mask (if mask not provided)\n",
        "        if self.mask is None:\n",
        "            self.mask = torch.triu(torch.ones((seq_len, seq_len)), diagonal=1).to(scores.device)\n",
        "            self.mask = self.mask == 1  # Convert to boolean mask\n",
        "\n",
        "        scores = scores.masked_fill(self.mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
        "        #print(scores) for debugging\n",
        "        # Apply softmax to obtain attention weights\n",
        "        attn_weights = F.softmax(scores, dim=-1)  # (batch_size, heads, seq_len, seq_len)\n",
        "\n",
        "        # Compute output\n",
        "        out = torch.matmul(attn_weights, V)  # (batch_size, heads, seq_len, dv)\n",
        "\n",
        "        # Concatenate heads\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.heads * self.dv)\n",
        "\n",
        "        # Final linear transformation\n",
        "        out = torch.matmul(out, self.WO.T)  # (batch_size, seq_len, dmodel)\n",
        "        # print(\"Size of output from Mask Attention layer is\\n\", out.size())for debugging\n",
        "        return out"
      ],
      "metadata": {
        "id": "uL8NHmHFLUZH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MHCA"
      ],
      "metadata": {
        "id": "nrDQbLIF334F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MHCA(nn.Module):\n",
        "    def __init__(self, dmodel, dq, dk, dv, heads):\n",
        "        super(MHCA, self).__init__()\n",
        "\n",
        "        self.dmodel = dmodel\n",
        "        self.dq = dq\n",
        "        self.dk = dk\n",
        "        self.dv = dv\n",
        "        self.heads = heads\n",
        "\n",
        "        # Initialize weights for Q, K, V, and output using the specified seeds\n",
        "        torch.manual_seed(43)\n",
        "        self.WQ = nn.Parameter(torch.randn(dq * heads, dmodel))\n",
        "\n",
        "        torch.manual_seed(44)\n",
        "        self.WK = nn.Parameter(torch.randn(dk * heads, dmodel))\n",
        "\n",
        "        torch.manual_seed(45)\n",
        "        self.WV = nn.Parameter(torch.randn(dv * heads, dmodel))\n",
        "\n",
        "        torch.manual_seed(46)\n",
        "        self.WO = nn.Parameter(torch.randn(dmodel, dv * heads))\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        # Query is the output from the masked attention sub-layer\n",
        "        # Key and Value come from the encoder output (which is fixed to a random matrix here)\n",
        "\n",
        "        # Linear transformations for Q, K, V\n",
        "        Q = torch.matmul(query, self.WQ.T)  # (batch_size, seq_len, dq * heads)\n",
        "        K = torch.matmul(key, self.WK.T)    # (batch_size, seq_len, dk * heads)\n",
        "        V = torch.matmul(value, self.WV.T)  # (batch_size, seq_len, dv * heads)\n",
        "\n",
        "        # Reshape Q, K, V for multi-head computation\n",
        "        batch_size = Q.shape[0]\n",
        "        seq_len = Q.shape[1]\n",
        "\n",
        "        Q = Q.view(batch_size, seq_len, self.heads, self.dq).transpose(1, 2)  # (batch_size, heads, seq_len, dq)\n",
        "        K = K.view(batch_size, seq_len, self.heads, self.dk).transpose(1, 2)  # (batch_size, heads, seq_len, dk)\n",
        "        V = V.view(batch_size, seq_len, self.heads, self.dv).transpose(1, 2)  # (batch_size, heads, seq_len, dv)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        dk = torch.tensor(self.dk, dtype=torch.float32)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(dk)  # (batch_size, heads, seq_len, seq_len)\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Compute output\n",
        "        out = torch.matmul(attn_weights, V)  # (batch_size, heads, seq_len, dv)\n",
        "\n",
        "        # Concatenate heads\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.heads * self.dv)\n",
        "\n",
        "        # Final linear transformation\n",
        "        out = torch.matmul(out, self.WO.T)  # (batch_size, seq_len, dmodel)\n",
        "        #print(\"Size of output from Cross Attention layer is\\n\", out.size())for debugging\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "2URLKYHT8i1x"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Implement the FFN and OutputLayer modules (same as the one you implemented for encoder)"
      ],
      "metadata": {
        "id": "HkWY1xuzbV1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FFN(nn.Module):\n",
        "    def __init__(self, dmodel, d_ff):\n",
        "        super(FFN, self).__init__()\n",
        "\n",
        "        # First linear layer maps dmodel -> d_ff\n",
        "        self.linear1 = nn.Linear(dmodel, d_ff)\n",
        "\n",
        "        # Second linear layer maps d_ff -> dmodel\n",
        "        self.linear2 = nn.Linear(d_ff, dmodel)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First linear layer followed by ReLU activation\n",
        "        x = F.relu(self.linear1(x))\n",
        "\n",
        "        # Second linear layer\n",
        "        out = self.linear2(x)\n",
        "        #print(\"Size of output from FFN layer is\\n\", out.size())for debugging\n",
        "        return out"
      ],
      "metadata": {
        "id": "WriFqQWHAGZm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OutputLayer(nn.Module):\n",
        "    def __init__(self, dmodel, vocab_size):\n",
        "        super(OutputLayer, self).__init__()\n",
        "\n",
        "        # Linear layer mapping dmodel -> vocab_size to predict token IDs\n",
        "        self.linear = nn.Linear(dmodel, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply linear layer to project to vocab size\n",
        "        out = self.linear(x)\n",
        "        #print(\"Size of output from Output layer is\\n\", out.size())for debugging\n",
        "        return out"
      ],
      "metadata": {
        "id": "sZ8mE5xQbzh2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Implement the final decoder layer."
      ],
      "metadata": {
        "id": "htg7Hx-Kb1_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, dmodel, dq, dk, dv, d_ff, heads, mask=None):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        # Multi-head Masked Attention sub-layer\n",
        "        self.mhma = MHMA(dmodel, dq, dk, dv, heads, mask=mask)\n",
        "        # Multi-head Cross Attention sub-layer\n",
        "        self.mhca = MHCA(dmodel, dq, dk, dv, heads)\n",
        "        # Position-wise Feed Forward Network\n",
        "        self.ffn = FFN(dmodel, d_ff)\n",
        "\n",
        "        # Layer normalization for each sub-layer\n",
        "        self.layer_norm_mhma = nn.LayerNorm(dmodel)\n",
        "        self.layer_norm_mhca = nn.LayerNorm(dmodel)\n",
        "        self.layer_norm_ffn = nn.LayerNorm(dmodel)\n",
        "\n",
        "    def forward(self, x, encoder_output):\n",
        "        # Multi-head Masked Self-Attention with residual connection and layer normalization\n",
        "        mhma_output = self.mhma(x)\n",
        "        x = self.layer_norm_mhma(x + mhma_output)\n",
        "\n",
        "        # Multi-head Cross Attention with residual connection and layer normalization\n",
        "        mhca_output = self.mhca(x, encoder_output, encoder_output)\n",
        "        x = self.layer_norm_mhca(x + mhca_output)\n",
        "\n",
        "        # Feed Forward Network with residual connection and layer normalization\n",
        "        ffn_output = self.ffn(x)\n",
        "        out = self.layer_norm_ffn(x + ffn_output)\n",
        "        #print(\"Size of output from the Decoder layer is\\n\", out.size())for debugging\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "y3KrOhIj_1YF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Create an embedding layer that takes in token_ids and return embeddings for the token_ids\n",
        "\n",
        " * Use seed value: 70"
      ],
      "metadata": {
        "id": "0on1cUNEcEt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embed(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super(Embed, self).__init__()\n",
        "\n",
        "        # Set the random seed for reproducibility\n",
        "        torch.manual_seed(70)\n",
        "\n",
        "        # Initialize the embedding layer\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Lookup the embeddings for the token_ids\n",
        "        out = self.embed(x)\n",
        "        #print(\"Size of output from Embedding layer is\\n\", out.size())for debugging\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "0a--wC-_Tf_W"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder\n",
        "\n",
        " * Implement the decoder that has `num_layers` decoder layers"
      ],
      "metadata": {
        "id": "Xr3nTQP5d2zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, dmodel, dq, dk, dv, d_ff, heads, mask=None, num_layers=1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # Embedding layer for the target token IDs\n",
        "        self.embed_lookup = Embed(vocab_size, dmodel)\n",
        "\n",
        "        decoder_layer = DecoderLayer(dmodel, dq, dk, dv, d_ff, heads, mask)\n",
        "\n",
        "        # Stack of decoder layers\n",
        "        self.dec_layers = nn.ModuleList([copy.deepcopy(decoder_layer) for _ in range(num_layers)])\n",
        "\n",
        "        # Output layer to project the decoder output to the vocabulary size\n",
        "        self.output_layer = OutputLayer(dmodel, vocab_size)\n",
        "\n",
        "    def forward(self, enc_rep, tar_token_ids):\n",
        "        # Get embeddings for the target token IDs\n",
        "        x = self.embed_lookup(tar_token_ids)\n",
        "\n",
        "        # Pass through each decoder layer\n",
        "        for dec_layer in self.dec_layers:\n",
        "            x = dec_layer(x, enc_rep)\n",
        "\n",
        "        # Final output layer to get the logits for the vocabulary\n",
        "        out = self.output_layer(x)\n",
        "        #print(\"Size of the final output from DECODER is\\n\", out.size())for debugging\n",
        "        return out"
      ],
      "metadata": {
        "id": "SYRrxMkpBzAe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Representation from encoder\n",
        "\n",
        " * Since all the decoder layers require the representation from the encoder to compute cross attention, we are going to feed in the random values (Note, it does not require gradient during training)"
      ],
      "metadata": {
        "id": "lyZYR7NkCYh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# do not edit this\n",
        "enc_rep = torch.randn(size=(batch_size,seq_len,embed_dim),generator=torch.random.manual_seed(10))\n",
        "print(enc_rep.size())"
      ],
      "metadata": {
        "id": "-rOysYMjB-af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca881b50-c43c-4e4a-bdd1-83860838328c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 8, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate the model"
      ],
      "metadata": {
        "id": "tpLKJVXnGbbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Decoder(vocab_size,dmodel,dq,dk,dv,d_ff,heads,mask=None)"
      ],
      "metadata": {
        "id": "aeG9CjI_GcsP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "aebVyW-9LhcP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train(enc_rep, tar_token_ids, label_ids, epochs=1000):\n",
        "    loss_trace = []\n",
        "    for epoch in range(epochs):\n",
        "        out = model(enc_rep, tar_token_ids)\n",
        "        out = out.view(-1, vocab_size)\n",
        "        target = tar_token_ids.view(-1)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(out, target.long())\n",
        "        loss_trace.append(loss.item())  # Store the loss value for visualization\n",
        "\n",
        "        # Print loss every 100 epochs\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print(f'Loss in epoch - {epoch + 1} is {loss.item()}')\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    # Plot the loss curve\n",
        "    plt.plot(range(epochs), loss_trace, label='Training Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss vs. Epochs')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "8npNWRJ7tWRc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Train the model for 1000 epochs"
      ],
      "metadata": {
        "id": "lXLdvUVKgU3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(enc_rep,token_ids,label_ids,1000)"
      ],
      "metadata": {
        "id": "svrd7-yHtpme",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 735
        },
        "outputId": "9200a5cf-0f46-43f6-edd0-ff81e80bdf35"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-b5d33902253e>:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  dk = torch.tensor(self.dk, dtype=torch.float32)\n",
            "<ipython-input-7-9b822234539e>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  dk = torch.tensor(self.dk, dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss in epoch - 100 is 1.9854390621185303\n",
            "Loss in epoch - 200 is 1.637133240699768\n",
            "Loss in epoch - 300 is 1.3524713516235352\n",
            "Loss in epoch - 400 is 1.1218345165252686\n",
            "Loss in epoch - 500 is 0.9193586111068726\n",
            "Loss in epoch - 600 is 0.7540795207023621\n",
            "Loss in epoch - 700 is 0.5999165773391724\n",
            "Loss in epoch - 800 is 0.47275200486183167\n",
            "Loss in epoch - 900 is 0.36953356862068176\n",
            "Loss in epoch - 1000 is 0.2794091999530792\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "  predictions = torch.argmax(model(enc_rep,token_ids),dim=-1)"
      ],
      "metadata": {
        "id": "M6PLInPW0JIF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc63912b-d3a7-4810-caf7-e7d9c26d607c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-b5d33902253e>:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  dk = torch.tensor(self.dk, dtype=torch.float32)\n",
            "<ipython-input-7-9b822234539e>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  dk = torch.tensor(self.dk, dtype=torch.float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The loss will be around 0.17 after 1000 epochs"
      ],
      "metadata": {
        "id": "TsA-osXehjEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# number of correct predictions\n",
        "print(torch.count_nonzero(label_ids==predictions[:,1:]))"
      ],
      "metadata": {
        "id": "hbnfyb5thxCP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdc50241-7e05-4d14-dbce-11392d13b39b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(68)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* THe number of correct predictions is close to 66"
      ],
      "metadata": {
        "id": "UXTPr2pd6tfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize total_num_parameters to zero\n",
        "total_num_parameters = 0\n",
        "\n",
        "# Loop through all named parameters in the model\n",
        "for name, parameter in model.named_parameters():\n",
        "    # Get the number of elements in the parameter tensor\n",
        "    num_params = parameter.numel()\n",
        "    # Print the parameter name and number of elements\n",
        "    print(f\"Parameter name: {name}, Number of parameters: {num_params}\")\n",
        "    # Add this to the total count\n",
        "    total_num_parameters += num_params\n",
        "\n",
        "# Print the total result\n",
        "print('Total number of parameters in the model, including the embedding layer, is:', total_num_parameters)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7laRDzp3RyQC",
        "outputId": "277e6fec-7840-4472-8b78-d4098f19b8bd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter name: embed_lookup.embed.weight, Number of parameters: 384\n",
            "Parameter name: dec_layers.0.mhma.WQ, Number of parameters: 1024\n",
            "Parameter name: dec_layers.0.mhma.WK, Number of parameters: 1024\n",
            "Parameter name: dec_layers.0.mhma.WV, Number of parameters: 1024\n",
            "Parameter name: dec_layers.0.mhma.WO, Number of parameters: 1024\n",
            "Parameter name: dec_layers.0.mhca.WQ, Number of parameters: 1024\n",
            "Parameter name: dec_layers.0.mhca.WK, Number of parameters: 1024\n",
            "Parameter name: dec_layers.0.mhca.WV, Number of parameters: 1024\n",
            "Parameter name: dec_layers.0.mhca.WO, Number of parameters: 1024\n",
            "Parameter name: dec_layers.0.ffn.linear1.weight, Number of parameters: 4096\n",
            "Parameter name: dec_layers.0.ffn.linear1.bias, Number of parameters: 128\n",
            "Parameter name: dec_layers.0.ffn.linear2.weight, Number of parameters: 4096\n",
            "Parameter name: dec_layers.0.ffn.linear2.bias, Number of parameters: 32\n",
            "Parameter name: dec_layers.0.layer_norm_mhma.weight, Number of parameters: 32\n",
            "Parameter name: dec_layers.0.layer_norm_mhma.bias, Number of parameters: 32\n",
            "Parameter name: dec_layers.0.layer_norm_mhca.weight, Number of parameters: 32\n",
            "Parameter name: dec_layers.0.layer_norm_mhca.bias, Number of parameters: 32\n",
            "Parameter name: dec_layers.0.layer_norm_ffn.weight, Number of parameters: 32\n",
            "Parameter name: dec_layers.0.layer_norm_ffn.bias, Number of parameters: 32\n",
            "Parameter name: output_layer.linear.weight, Number of parameters: 384\n",
            "Parameter name: output_layer.linear.bias, Number of parameters: 12\n",
            "Total number of parameters in the model, including the embedding layer, is: 17516\n"
          ]
        }
      ]
    }
  ]
}